{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIAW5aRFGVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7cbf786-fb10-4da8-a2d1-5231105c8d9e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orArDWJGFNSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "748351e2-99ed-406d-9df5-d7f76b33df30"
      },
      "source": [
        " import nltk\n",
        " nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8Zw_F2FPnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "09974fef-8c5b-4e26-bcb2-6b1c8b9fdb4a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers as initializers, regularizers, constraints\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from nltk import tokenize\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOWFiufMFV9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0d72ef67-7efd-4d45-f3ef-1959a8e8c281"
      },
      "source": [
        "data_frame = shuffle(pd.read_csv('drive/My Drive/Colab Notebooks/dataset/yelp2014train.ss',sep='\\t\\t', names=[\"user\", \"business(product)\", \"rating\", \"review\"])).reset_index()\n",
        "data_frame = data_frame[0:60000]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njqa4NbPFXLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = data_frame[['review', 'rating']]\n",
        "rating = df['rating']\n",
        "review = df['review']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q8tJUj0TDS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe0e41d2-f8a1-405f-c24f-0dc8d4bcd07e"
      },
      "source": [
        "rating.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuz47UN-HjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ5EXBsWFaCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"\\\\\", \"\", string)    \n",
        "    string = re.sub(r\"\\'\", \"\", string)    \n",
        "    string = re.sub(r\"\\\"\", \"\", string)    \n",
        "    return string.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TJRyd7wGaUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = []\n",
        "for idx in range(df.review.shape[0]):\n",
        "    text = clean_str(df.review[idx])\n",
        "    texts.append(text)\n",
        "\n",
        "# text = rating.apply(lambda x : clean_str(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXD5SpJiNxDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = pd.get_dummies(rating)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JXWVEdNQbCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,labels, test_size = 0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZIJNlFWQnTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "89cef1b9-b945-4908-88c8-5ae00da74e0e"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, SpatialDropout1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('drive/My Drive/Colab Notebooks/weights/LSTM_model.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto') \n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=512, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 563s 12ms/step - loss: 1.3404 - acc: 0.4097 - val_loss: 1.1662 - val_acc: 0.4607\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 564s 12ms/step - loss: 1.1125 - acc: 0.4917 - val_loss: 1.1103 - val_acc: 0.4708\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 569s 12ms/step - loss: 1.0185 - acc: 0.5365 - val_loss: 1.0740 - val_acc: 0.5078\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 564s 12ms/step - loss: 0.9458 - acc: 0.5810 - val_loss: 1.0561 - val_acc: 0.5243\n",
            "Epoch 5/20\n",
            "13312/48000 [=======>......................] - ETA: 6:20 - loss: 0.8662 - acc: 0.6146"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8plBU_Nbo0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}